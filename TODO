- branchtrans appears broken; no matter what the target-stat is, each
  branch target keeps getting recomputed.

  Needed to treat already computed branch targets differently

+ Possibly change makeres-branch so that the branch source is actually
  the list form providing the branching.  At the moment, the algorithm
  assumes that there will be a single target which branches on a list
  of possibilities, and that this target must be referred to somewhere
  along a chain of branching sources.  However, what would be better
  would be to have parallel branches be able to access each other's
  targets during the branching, so this would require that the source
  be allowed to be the branch list form itself, or at least have this
  behavior emulated.

+ Provide some mechanism for treating makeres target-table
  transformations as plugins so that initialization and possibly even
  transformation ordering in the pipeline can be automated so users
  don't have to manage minutia.

- Potentially useful makeres transformation: Operators which allow
  branching.  Example: Creating histograms with different binnings.
  The branching operator could allow specification of a list of
  different branch values and an operator which would refer to the
  particular branch value in a branch instance, but the code would
  remain the same otherwise.  The branch operator would return a hash
  table mapping from the branch value to the corresponding computed
  result.  The branch transformation could create a result target for
  each branch so that e.g. table transformations would still work
  properly.  There could be a function #'branch (potentially private
  to the transformation package) which would take an id symbol and
  return the branch value for that particular branch instance.  That,
  or have a system of hash-tables.

- Bug in in-project: Calling in-project repeatedly on the same project
  id actually erases the target definitions.

  Fixed by creating function in-project-fn

+ makeres-table may have more problems.

  * On one occasion, after a computation failed, the source table was
    not logged although some of the reductions were logged.  This led
    to circular dependencies and compres would exhaust the stack.  The
    solution was to call setresfn on the source table ID using any
    table and then call save-target.  Once all the source tables were
    logged, the dependencies worked correctly and computation
    continued.

  * Also, when working on my phd analysis, after a computation fails
    and I correct the bug, some reductions which presumably should
    have been calculated already need recomputing.  This and the above
    potential bug may have the same source, I suspect some issues with
    the order of calls to setresfn, or maybe in setresfn itself.

+ Update documentation on tables due to changing the functionality of
  table field names

- Extend tables so that field names are general keyword symbols

  - hdf-tables and csv-tables are working with general keyword symbols
    as well as serialization of histograms.

  - ntuple-table and plist-table may need modification still

+ Allow any HDF5 compound type to be used with hdf-tables, not just
  those which are created from structures.  The field offsets are
  dependent on the compiler for structures, but the HDF5 files contain
  the offset information, so in principle any HDF5 type could be
  supported.  This would require a complete rennovation of the
  hdf-typespec utility, or perhaps a complete abandonment in favor of
  custom code just for hdf-tables.

* See makeres/ToDo.txt for makeres specific tasks

- Renovate logres so that results can be loaded/saved dynamically.

  There are two broad approaches:

  1. Find a naming/organization scheme on disk which is static;
     i.e. each target is algorithmically given a name which is unique
     to that target ID form, and each target handles its own contents
     by storing itself as either a directory or a file.

  2. Keep flat storage strategy, but implement condensing algorithm
     which renames the files such that the minimum maximum value is
     used; at the moment, naively keeping flat storage would result in
     ever growing index numbers due to old targets being removed while
     new targets keep acquiring larger index values.

  I'm leaning towards 1 due to being conceptually easier to work with
  and not all that difficult to implement.  The downside to 1 is that
  old result logs will either need to be converted or recomputed.

  Went with 1, used the target id as the pathname relative to the
  version path.

+ Fix read-histogram so that output from C/C++ code which does not use
  structs can still be used.

  At the moment, structs are used and the offets from a C struct
  placed in the HDF5 type since type conversion for compound types is
  built on top of CFFI's defcstruct.

  The only other options would be to include offset information in a
  compound typespec and then update the HDF5 -> typespec functions as
  well as any which process typespecs.

% Create raw-hdf-table and raw-hdf-table-chain which remove the
  automatic type conversion from HDF5 types to Lisp, and require the
  user to use foreign object interfaces from e.g. CFFI to analyze
  data.  This seems to be necessary for large datasets since, as per
  the results from the benchmarks below, ROOT with C++ is 2 orders of
  magnitude faster than the current Lisp implementation.

  NOTE: The raw versions are pending for deletion due to new benchmark
  results; type conversion isn't the bottleneck.

- Conduct detailed benchmarks of Lisp performance vs. C++ with ROOT or
  HDF5 for processing datasets of varying size and structure, varying
  number of histograms to fill, etc., both for makeres generated code
  and for fully optimized Lisp.

  Tentative results: that ROOT with C++ runs 2 orders of magnitude
  faster than either the macro or compiled benchmarks in Lisp which
  use the low level libraries, and type declaration doesn't appear to
  aid much at all.

  Final results: C++ with ROOT is two orders of magnitude faster than
  cl-ana functionality (additional type declarations don't have any
  effect), but less than a factor of 2 away from raw HDF5 CFFI
  function calls from within Lisp.  cl-ana histograms are a
  significant bottleneck even with type declarations.  C++ with HDF5
  and naive histograms is even faster than ROOT.

- work-path function which returns paths relative to the current
  working directory, ensure-directories-exist being called to define
  subdirectories automatically

- Single function/macro for defining a makeres+logres project along
  with common settings.

+ Update copyright to be 2013-2015

logres:

- If logged target form is not equal to the one loaded into the Lisp
  image, then propogate the need to compute any targets which depend
  on that target.  At the moment, I think I'm just not loading the
  target, but in principle if you change the form (and value) of a
  target but load targets which actually depend on it, then you'll
  load the wrong values which actually need recomputing as well.

- Log target form so that current form can be checked against the one
  used to compute the result logged.  This removes the need for the
  user to keep track of which targets have been updated if the results
  were not saved or if multiple analysis versions are being used.
  Updates to libraries will still be problematic however.

- Each target and logged file should come with a timestamp so that
  versions of targets and files can be compared.  This would be useful
  for e.g. sending files to another computer and only updating the
  files which have differing timestamps as well as only saving targets
  which have changed when saving a project.

+ Use SHA1 sums for checking work files

  This is partially completed, but I need to find a way to easily
  track sha1 sums for files.  At the moment, it's turning into a
  monolithic monster whereas I would rather have operators provided
  which automate the management of the sha1 sums.

+ Logging functionality may need to be partially supported by graph
  transformations so that targets can be saved to disk and cleared
  from memory to avoid memory bottlenecks on low-memory systems.  It's
  already taking ~ 1.5 GB for my PhD analysis just to store all the
  histograms etc. in memory.

makeres:

- Add tabletrans operator which allows multiple targets to be defined
  and computed as a block.  Would greatly assist in allowing
  e.g. multiple maps over the same computed list, fitting data (since
  functions themselves cannot be logged but the parameters can,
  requiring multiple targets per fit).

  Could have a macro defresblock defining a result block.  Would take
  the ids of the computed results as arguments and would define
  temporary versions of the targets.  The tabletrans transformations
  would take care of providing e.g. the ability to refer specifically
  to which target you want to incrementally compute throughout the
  block.

  (COULD STILL DO THIS) This could actually provide a better platform
  for implementing the table transformations as well, as the merging
  of table passes ultimately computes multiple targets at once,
  meaning that at least some of the work currently done by
  makeres-table could be moved to this new tabletrans library.

+ Logging functionality may best be provided in the compiled function
  of compres.  Each target would have as its path its res id.  Each
  target would get its own directory, and inside a target's directory
  would be a timestamp file as well as the saved information from the
  target.  Each kind of target could have its own methods for storage,
  be it a single binary file or multiple files along with an index
  file.  Deleting a result could then automatically delete the stored
  result as well if desired.

* Some way to control when and how transformations are loaded into the
  pipeline that doesn't require the user to know the exact ordering
  required.  As it stands, you have to know that e.g. macro
  transformations are required for table transformations, and that the
  transformed macros should be expanded prior to table
  transformations.

makeres-macro:

+ Add ability to define functions and still have dependencies tracked.
  This should be done simply by defining a new function definition
  operator, define-res-function, which will store its dependencies in
  a table.  Then, the makeres-macro tabletrans function will use this
  information as it code walks, looking for any calls to a
  res-function in a body and adding dependencies to the affected
  targets accordingly.

makeres-table:

+ Source table definition with bootstrapping.  At the moment, the user
  is tasked with opening source tables and handling the situation when
  the source material for a table is not present.  There should be an
  operator, perhaps stab, which allows the definition of a source
  table.

+ Perhaps renaming the operator tab to ptab.  If the above stab
  operator is implemented, then having stab, ptab, and ltab is more
  symmetrical and potentially easier to remember than having stab,
  tab, and ltab, since physical tables won't be as special once a
  separate operator is made for source tables.
