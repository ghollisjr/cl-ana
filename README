cl-ana is a free (GPL) library of Common Lisp code for doing data
analysis; specifically it has to at least support what I'm doing right
now: analyzing particle accelerator event data.

Whenever possible, features are implemented via generic functions so
that users can extend cl-ana to whatever they want to do.

So far the functionality this library provides includes:

* Tabulated data: Supports data tables read-from and written-to HDF5,
  ntuples, comma separated value (CSV) files, and plists for
  all-in-memory operation.  Adding a new table type is as easy as
  extending the table class and defining 4 functions for the table
  type.

* Histograms: Supports contiguous and sparse histograms of arbitrary
  dimensions.  Provides functional access to histograms via mapping
  (which allows reducing) and filtering.

* Nonlinear least squares fitting: Allows plain-old lisp functions to
  be fitted to data using the GNU Scientific Library (GSL); infers the
  number of fit parameters the function takes from the initial
  parameter guess.  Can fit against alists of data & histograms and is
  easily extended to allow fitting against other types by defining a
  single function for the new type.

* Plotting: Uses gnuplot to plot histograms, data samples, plain-old
  lisp functions, and strings interpreted as formulae.

* Generic math: Common Lisp doesn't provide user-extendable math
  functions; cl-ana provides its own versions of the basic math
  functions CL gives you but with the ability to extend them for
  whatever types you want.  Also provides use-gmath which easily adds
  generic-math's symbols to a package even if you already use the
  common-lisp package.  Already provided are extensions to the generic
  math functions for error propogation, quantities (values with
  units), and treating CL sequences as tensors with all the usual math
  functions being applied element-by-element in a MATLAB/GNU Octave
  fashion.

Also included are various utilities which have use in a variety of
places.

The main principles of the project are:

1. Conceptual clarity and documentation.  These are often neglected in
   software development, to the point where reading code can cause one
   to drink.  Conceptual clarity refers to the way in which code is
   written and the way in which algorithms are implemented: A slightly
   slower but easier to understand implementation is favored above a
   clusterfuck of bit shifts.  Documentation should always be provided
   for any feature along with example usages--ESPECIALLY with example
   usages, as these are sometimes more helpful than the actual
   documentation.

2. Modularity/Bottom-up design.  Whenever two components have a common
   feature/function/dependency, this commonality should be placed in a
   separate sublibrary.  To limit sublibrary number explosion, this
   should be done in conjunction with point 1 preserving conceptual
   clarity.  For example list utilities should be a sublibrary for
   general purpose list functions.  Further: If a feature can be
   provided by either a set of utility functions or a type heirarchy,
   strong preference should be given to the utility functions
   approach; i.e. one should have to argue long and hard before
   stratifying things into classes.

3. Lispyness.  Whenever possible, already established motifs from LISP
   programming practices should be used.  This goes for naming
   conventions, access macros, and the general desire to provide at
   least functional access to things.

Each sublibrary should go in its own directory and come with its own
.asdf file so that one can choose any subset of functionality to use
from the library.

As you will see in reading the code, I've tried to keep everything
well documented.  I place a high emphasis on documentation since I
know how easy it is to fall out of practice.  The last thing I want is
for the usual cargo-cult around old code to emerge.

I'd like to make a disclaimer that much of the code I've written has
been part of my own personal development as a LISP programmer; this is
my first non-trivial project with LISP, and coming from a C++
background I've had to learn quite a few things along the way.  This
means that there may be some dark corners of the code which need help
from more experienced coders/myself at a later time.  In addition, I
haven't used any general testing framework.  (To be honest I haven't
needed one either as I've done the development in a highly bottom-up
way, testing everything as I write it.)  In short this is a work in
progress.

There is one major piece of this project that is still missing: a
complete documentation/tutorial for using the tools provided.  This is
the next thing on my plate, and I've only been postponing it while I
learn how to use texinfo.

The dependencies for this project are:

* HDF5 (http://www.hdfgroup.org/HDF5/)
* GSL (http://www.gnu.org/software/gsl/)
* GSLL (http://common-lisp.net/projects/gsll/)
* Alexandria (http://common-lisp.net/project/alexandria/)
* iterate (http://common-lisp.net/project/iterate/)
* cl-csv (https://github.com/AccelerationNet/cl-csv)
* gnuplot (http://www.gnuplot.info/)
* gnuplot_i (http://ndevilla.free.fr/gnuplot/).

Most of the LISP dependencies can be installed via quicklisp
(http://www.quicklisp.org/).

gnuplot_i was written by N. Devillard <ndevilla@free.fr>, released to
the public domain, and is a no-nonsense gnuplot session manager
written in C.  There is no way to get information from the session
into LISP from gnuplot_i, but it saved the time writing my own session
manager.

I use SBCL (http://www.sbcl.org/) almost exclusively; however, I also
intentionally try to ensure that all the code only assumes what the CL
standard provides.  Anytime implementation-specific functionality is
needed I try to use third party libraries for this.

So far some of the features implemented are:

--------------------------------------------------
1. Generic mathematics

Common lisp comes out of the box lacking generic mathematics; i.e. one
cannot simply extend the +, -, /, *, sqrt, etc. functions.  I looked
for libraries which already did this, and found among them Antik.
However: Antik's structure was heavily biased (IMO) towards grids &
arrays.  So, I decided to just build the basic mathematical generic
functions which can sit atop the built-in CL functions as well as
anything that libraries like Antik provide.

This resulted in the development of a simple error propogation library
among other benefits for histogramming, data fitting, linear algebra
(no I did not re-implement this), etc.

--------------------------------------------------
2. Error propogation & Quantities

The error-propogation library provides err-num numerical objects which
automatically perform error propogation during a computation.  This
was done via specializing on the generic math functions as provided by
the generic math library.

It also provides the reader macro #e() which allows one to write

#e(x y ...)

to mean

x +- y +- ....

The structure is recursive so that each value (other than the first)
is the error in the preceding value.  The generic math library is
implemented in such a way that the propogation is done correctly at
all levels of error (precluding bugs that is).

As a similar physics-based numerical concept, I have implemented
physical quantities (values with dimensions like length, mass, etc.)
as well. Quantities are represented by a CLOS class, with conversions
from numbers, err-nums and keyword symbols (used to represent units)
into quantities.  Libraries like Antik have already implemented
quantities, but I chose to approach the problem from a different
direction by defining generic functions specializing on keyword
symbols, numerical values, and the quantity type in order to keep
everything on the functional side as opposed to the reader macro side
of things.

Utilities are provided for easy definition of new units & constants;
the base unit system is S.I.

Quantities and error propogation play nice together; as an example:

(* #e(5 1.3) :meter)

would denote (5 +- 1.3) meters, and subsequent operations would have
error propogation included in the calculation.

I have thought to just define a function +- which would replace the
#e() reader macro; this may happen in subsequent development.

--------------------------------------------------
3. Tables

Tables are 2 dimensional data storage abstractions (think of the awk
Unix utility).  A table is composed of a number of rows.  Each row is
composed of a number of fields.  Each row must have the same number
and kinds of fields; i.e. tables are rectangular.

Tables are required to provide at a minimum sequential reading
from/writing into, but a table when opened for reading is only
required to available for reading, and the same goes for tables opened
for writing, though some table types may specifically provide two-way
access.

(I intend to provide higher level table management functionality that
abstracts away this limitation by automatically managing the physical
table objects behind the scenes.)

For writing, one can modify the fields of a row and then commit the
row into the table.  As a convience, this can be done as a one-step
process using the table-push-fields macro which uses a let-like syntax
to denote the values of the table fields.

For reading, one can use either the table-reduce function or the
do-table macro.  do-table creates a loop body inside which the field
values for each row are already known; all you have to do is reference
a (marked) field variable name and it will be read into memory each
iteration of the loop.  table-reduce is used to implement do-table and
is useful for when one wants to programmatically select fields from
the table for instance.

(I am aware of the Data Table library
(https://github.com/AccelerationNet/data-table) already available;
however, using this along with HDF5 files seemed like a potentially
messy endeavor.  So I decided to implement tables as a higher-level
abstraction, of which the data-table type from the above library could
be an instance.)

So far I've implemented the following table types:

hdf-table: Provides access to HDF5 datasets.  The access is limited to
1-dimensional datasets with compound datatypes as the dataset datum
type.  Allows random read access.  Related is the read-only
hdf-table-chain object which chains together a list of files
containing the same type of HDF5 dataset as a single table, managing
files and reading data into memory as needed.  Using an
hdf-table-chain as opposed to the more general table-chain described
below has the advantage of knowing how many rows are in the total
table chain before one traverses them.

csv-table: Provides access to CSV (Comma Separated Values) files.
Convenient for interfacing with spreadsheet data.  Can be read or
written.

ntuple-table: Provides access to ntuple datasets as implemented by GSL
(uses GSLL).  Can be read or written, though the ntuple format does
not include any information on the type of data it stores, so you have
to keep track of it yourself.

plist-table: Creates a readable table object from a list of property
lists (known as plists in LISP terminology) where each plist is
interpreted to be a row.  Handy for small-scale in-memory computation;
I used it for debugging mainly.

table-chain: Chains together tables of any type for reading, provided
they have the same row structure.  Could be handy for implementing the
higher-level table management functionality.

reusable-table: Wraps a lower-level table for reading in such a way
that it can be repeatedly read in its entirety without need to re-open
the table.  The disadvantage is that it is a little clumsy to access
the internal table object directly; the advantage is that one can
easily complete multiple passes over the table.

--------------------------------------------------
4. Histograms

Histograms are currently provided in two types: Contiguous and Sparse.

Contiguous histograms store histogram bin values in-memory in a
recursive array of arrays of arrays ..., what I implement in the
tensor sublibrary.  This is inefficient in memory when there are many
empty bins, but is efficient in computing time for arithmetic
operations, integration, etc.

Sparse histograms are implemented using hash tables so that empty bin
values are not stored.  This is efficient in memory when the data is
sparse but inefficient for arithmetic operations--so much so that at
the moment I neglect to implement these functions.

The operations common to both histogram types are:

* Insertion: Inserting values into a histogram is done statefully for
  the sole reason of efficiency.

* Mapping: One can map a function over the bins of a histogram,
  returning a new histogram.

* Filtering: Filtering makes use of mapping to exclude bins from the
  resulting histogram.  Useful for making cuts/selections on data
  samples.

* Integration over any number of dimensions for subranges or the
  entire dimension.

* Projection onto any selection of dimensions; this is just
  integration over all the non-selected dimensions.

* Conversion between the two types of histograms (contiguous <-->
  sparse).  This is useful for when you want to project a high
  dimensional sparse histogram onto only a few dimensions, making it
  better to have a contiguous histogram.

--------------------------------------------------
5. Fitting

Specifically, nonlinear least squares fitting as implemented by GSL.

The GSL library is supposed to implement the fitting without need for
an analytic Jacobian, but using GSLL I can't seem to figure out how to
use it/there is a bug in the interface or implementation, since every
time I call the appropriate function it fails with a memory fault
while inside a foreign function call.

However, the analytical Jacobian version works fine, so I just
implemented a higher-order function which creates the Jacobian
function given an input function.  This seems to remedy the situation
fine; I haven't seen any performance or accuracy issues so far in the
testing.

Fitting is done using lisp functions instead of special fit-able
function objects.  Some things about the function are inferred from
the other arguments, for example the number of fit parameters is
inferred from the initial parameter guesses you provide the fit
routine.

To integrate your data container with the fitting routine, you have to
specialize on a method which generates an alist mapping the
independent data values to the dependent ones, but that's all that's
required.

--------------------------------------------------
6. Lorentz vectors/boosts

I've implemented a simple lorentz vector and boosting library.

It's integrated into the generic math framework as well, so some of
the functions such as +, -, *, / will work.

Boosting is done by creating a boost matrix and then applying the
matrix to the lorentz vector; there are convenient functions written
to do this.  It seemed more efficient to create a single boost matrix
and then multiply many lorentz vectors instead of re-creating the
matrix every time you want to boost a single vector.

--------------------------------------------------
7. Plotting

Plotting is largely complete, though there are still a few things
which require direct gnuplot commands.

The structure of the gnuplot objects are mostly mimicked as CLOS
classes, and on top of these convenience functions and generic
functions are defined and specialized allowing one to plot LISP
functions, strings representing formulae, data in the form of alists,
histograms (1-D and 2-D; 2-D histograms are plotted with color as the
z-axis).

When using the full gnuplot object structure, the resulting code looks
much like a markup language for making plots; this is very useful for
quickly getting an idea of what is being plotted and can
simultaneously make use of programmatically generated plot structure.

--------------------------------------------------
Appendix: Miscellaneous libraries

In the process of creating the other functionality, I've had to create
several small utilities & libraries.

For example, there a number of x-utils libraries like list-utils,
string-utils, functional-utils, macro-utils, etc.  These provide small
functionality in the appropriate subdomains.

The memoization library provides a new function definition operator
defun-memoized which defines a memoized function.  These functions
remember previous return values so that lengthy computations/tree-like
recursive patterns can be efficiently computed.  It is implemented via
hash tables.

The tensor library generalizes the idea of mapping to sequences of
arbitrary structural depth, e.g. lists of lists, or arrays of lists of
arrays of ....  It also provides several useful functions which can be
defined on top of the tensor-map function, such as tensor-+, tensor--,
and other arithmetic operations as well as tensor contraction, which
turned out to be a fairly messy operation (you're welcome).

The typespec library (and hdf-typespec) provides typed data
descriptions and conversions that can be used with CFFI.  Raw CFFI
does a good job for basic tasks, but I found it difficult to copy
arbitrarily deep structures automatically, so I implemented this
functionality as part of typespec.  hdf-typespec provides the ability
to convert between HDF5 data types and CFFI types via typespecs.  This
allows the generation of cstruct types after reading the HDF5 data
type from an HDF file.

Lastly there are the foreign function interfaces I've written to
either provide interfaces with foreign libraries or extend/repair the
ones provided by other libraries I depend on, such as GSLL.
